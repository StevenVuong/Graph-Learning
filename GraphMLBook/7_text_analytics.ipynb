{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics and NLP Using Graphs\n",
    "\n",
    "Use a tagged corpus for:\n",
    "- Supervised: Classification models to classify documents in pre-determined topics\n",
    "- Unsupervised: Community detection to discover new topics\n",
    "\n",
    "Chapter covers:\n",
    "- Provide overview of dataset\n",
    "- Understand concepts and tools in NLP\n",
    "- Create graphs from a corpus of documents\n",
    "- Build a document topic classifier\n",
    "\n",
    "Dataset:\n",
    "\n",
    "Reuters-21578 news articles published in newswire in 1987. Has a very skewed distribution, so will use a modified version: ApteMod that has a smaller skew distribution and consistent labels between train and test sets. Each document has a set of labels that represents its content, thus is a perfect benchmark for testing supervised and unsupervised algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame([\n",
    "    {\"id\": _id, \"clean_text\": reuters.raw(_id).replace(\"\\n\", \"\"), \"label\": reuters.categories(_id)} # remove newline characters\n",
    "    for _id in reuters.fileids()\n",
    "]).set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[10][\"clean_text\"], corpus.iloc[10][\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "len(Counter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90 different topics with large class-imbalance, with 37% of documents in most common and 0.01% in each offive least common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "An nlp technique is to look for the most common words (stopwords) and build a score based on its frequency. However, there are many libraries that allow us to infer more elaborate logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "import numpy as np\n",
    "\n",
    "def getLanguage(text: str):\n",
    "    try: \n",
    "        return langdetect.detect(text)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "corpus[\"language\"] = corpus[\"clean_text\"].apply(getLanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many languages other than English\n",
    "# there may be some documents that are short or have a strange structure, so not actually news articles\n",
    "corpus[\"language\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using fasttext to detect language\n",
    "!curl -w GET https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz > lid.176.ftz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "m = fasttext.load_model(\"lid.176.ftz\")\n",
    "def getLanguage(text: str):\n",
    "    return m.predict(text)[0][0].replace(\"__label__\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"language\"] = corpus[\"clean_text\"].apply(getLanguage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[corpus[\"language\"]==\"ja\"].iloc[5][\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to text\n",
    "corpus[\"parsed\"] = corpus[\"clean_text\"].apply(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parsed object has several fields due to many models being combined to a single pipeline, these provide different levels of text structuring:\n",
    "-  **Text segmentation and tokenisation**: Aims to split a document into periods, sentences and single words/tokens. Leverages punctuation, blank spaces, newline characters for segmentation. Spacy works fairly well but in practice may require custom rules, such as separation based on hashtags for tweets (TweetTokenizer) etc..\n",
    "-  **Part-of-Speech Tagger**: Associate each token with PoS tag, its grammatical type - nouns, verbs, adjectives etc.. This mode lhas been trained on previous actual data\n",
    "-  **Named Entity Recognition (NER)**: Trained to recognise nouns that appear in text, e.g. Organisation, Person, Geographic location etc.. Usually trained on large, tagged dataset that learn common patterns/structures from\n",
    "-  **Dependency Parser**: Infers relationships between tokens within a sentence, can build a syntax tree of how words are related\n",
    "-  **Lemmatizer**: Reduces words to common root; reduce word to more stable form for easier processing. Or **Stemmers**: Remove last part of word to reduce to main part of word only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.loc[\"test/14832\"][\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice way to utilise entities in text\n",
    "displacy.render(corpus.loc[\"test/14832\"][\"parsed\"], style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "corpus[[\"clean_text\", \"label\", \"language\", \"parsed\"]].to_pickle(\"corpus.p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation\n",
    "\n",
    "Two kinds of graphs from the corupus of documents and information we extracted in the previous:\n",
    "-  **Knowledge based graphs**: Subject-verb-object (triplet) relation will be encoded to build a semantic graph\n",
    "-  **Bipartite graph**: Link documents with entities/keywords appearing therein\n",
    "\n",
    "### Knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subject_object_extraction import findSVOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[\"triplets\"] = corpus[\"parsed\"].apply(lambda x: findSVOs(x))\n",
    "corpus.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = []\n",
    "for _id, triplets in corpus[\"triplets\"].iteritems():\n",
    "    for (source, edge, target) in triplets:\n",
    "        edge_list.append({\"id\": _id, \n",
    "        \"source\": [x.lemma_ for x in nlp(source)][0].lower(), \n",
    "        \"target\": [x.lemma_ for x in nlp(target)][0].lower(), \n",
    "        \"edge\": [x.lemma_ for x in nlp(edge)][0].lower()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.DataFrame(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common are basic predicates\n",
    "edges[\"edge\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can create knowledge graph with networkx utility function\n",
    "import networkx as nx\n",
    "\n",
    "G=nx.from_pandas_edgelist(edges, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "len(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistribution(serie: pd.Series, nbins: int, minValue=None, maxValue=None):\n",
    "    _minValue=int(np.floor(np.log10(minValue if minValue is not None else serie.min())))\n",
    "    _maxValue=int(np.ceil(np.log10(maxValue if maxValue is not None else serie.max())))\n",
    "    bins = [0] + list(np.logspace(_minValue, _maxValue, nbins)) + [np.inf]\n",
    "    serie.hist(bins=bins)\n",
    "    plt.xscale(\"log\")\n",
    "\n",
    "def graphSummary(graph, bins=10):\n",
    "    print(nx.info(graph))\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(1,2,1)\n",
    "    degrees = pd.Series({k: v for k, v in nx.degree(graph)})\n",
    "    plt.yscale(\"log\")\n",
    "    plotDistribution(degrees, bins)\n",
    "    try:\n",
    "        plt.subplot(1,2,2)\n",
    "        allEdgesWeights = pd.Series({(d[0], d[1]): d[2][\"weight\"] for d in graph.edges(data=True)})\n",
    "        plotDistribution(allEdgesWeights, bins)\n",
    "        plt.yscale(\"log\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphSummary(G, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.log10(pd.Series({k: v for k, v in nx.degree(G)}).sort_values(ascending=False)).hist()\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at edges for 'lend'\n",
    "e = edges[(edges[\"source\"]!=\" \") & (edges[\"target\"]!=\" \") & (edges[\"edge\"]==\"lend\")]\n",
    "\n",
    "G=nx.from_pandas_edgelist(e, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise\n",
    "import os\n",
    "\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "pos = nx.spring_layout(G, k=1.2) # k regulates the distance between nodes\n",
    "\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos, font_size=12)\n",
    "\n",
    "# plt.show()\n",
    "# plt.savefig(os.path.join(\".\", \"KnowledgeGraph.png\"), dpi=300, format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63383c2a68c062c071f75888e0186e64940c33bbd30fa02b08cd99ee71ee0b47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
