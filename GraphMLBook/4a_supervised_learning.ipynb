{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Graph Learning\n",
    "\n",
    "**Supervised learning** represents the majority of practical **machine learning** tasks. Thanks to more active and effective data collection, it is imore common to deal with labeled datasets. This also applies to graphs, where labels can be assigned to nodes, communities and structures so that we learn some mapping function between the input and label.\n",
    "\n",
    "Code available at: https://github.com/PacktPublishing/Graph-Machine-Learning/tree/main/Chapter04\n",
    "\n",
    "In SL, a training set has a sequence of ordered pairs *(x, y)* where x is a set of input features and y is the output label assigned to it, we then want to learn the mapping function of each *x* value to each *y* value. In some situations we have a smaller dataset of labeled instances and a larger set of unlabeled instances. Here, **semi-SL (SSL)** is proposed, where algorithmns learn dependencies of available labels to learn prediting functions for unlabeled samples. There are various algorithm types:\n",
    "-  feature-based methods\n",
    "-  shallow embedding methods\n",
    "-  regularisation methods\n",
    "-  graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4_1](./figures/4_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature-based methods\n",
    "\n",
    "Simple and powerful method for ML on graphs: consider encoding function as a simple embedding lookup. One simple way to do this is to exploid graph properties, and we know that graphs can be described by (exploiting) structural properties so important information \"encoding\" from the graph itself. A shallow approach acts in two steps:\n",
    "1.  Select a set of *good* descriptive graph properties (e.g. avg. degree length, global efficiency etc..)\n",
    "2.  Use such properties as input or a traditional ML algorithm\n",
    "\n",
    "Unfortunately, there is no general definition of *good* descriptive properties, and their choice strictly depends on the specific problem to solve. \n",
    "\n",
    "Steps: \n",
    "1.  Convert StellarGraph to numpy adj matrices (networkx) and convert labels from Pandas series to numpy array.\n",
    "2.  Compute global metrics to describe each graph, e.g. num edges, avg. cluster coefficient, global efficiency (can compute graph metrics with networkx)\n",
    "3.  Exploit sckkit-learn to create train and test sets\n",
    "4.  Train a ML alg, choose support vector machine (SVM), trained to minimise the difference between the predicted labels and the actual labels\n",
    "\n",
    "For StellarGraph PROTEINS dataset, achieve about 80% F1-score, quite good for naive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "graphs, graph_labels = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from StellarGraph to numpy adj matrices\n",
    "adjs = [graph.to_adjacency_matrix().A for graph in graphs]\n",
    "\n",
    "# convert labels from pd.series to np array\n",
    "labels = graph_labels.to_numpy(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute global metrics to define each graph\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "metrics = []\n",
    "for adj in adjs:\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    \n",
    "    # basic properties\n",
    "    num_edges = G.number_of_edges()\n",
    "    \n",
    "    # clustering measures\n",
    "    cc = nx.average_clustering(G)\n",
    "    \n",
    "    # efficiency measure\n",
    "    eff = nx.global_efficiency(G)\n",
    "    \n",
    "    metrics.append([num_edges, cc, eff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(metrics, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[116, 0.4690476190476191, 0.29735760384740045]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7514970059880239\n",
      "Precision 0.7777777777777778\n",
      "Recall 0.8413461538461539\n",
      "F1-score 0.8083140877598153\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy', accuracy_score(y_test, y_pred))\n",
    "print('Precision', precision_score(y_test, y_pred))\n",
    "print('Recall', recall_score(y_test, y_pred))\n",
    "print('F1-score', f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Shallow embedding methods\n",
    "\n",
    "Subset of graph embedding methods that learn node, edge orgraph representation for a finite set of input data. Cannot be applied to other instances different from ones used to train the model. \n",
    "\n",
    "The main difference between unsupervised and supervised embedding models is the task they attempt to solve. If unsupervised shallow embedding algorithms try to learn a good graph/node/edge representation to build well defined clusters, supervised algorithms try to find the best solution for a prediction task, such as graph/node/edge classification.\n",
    "\n",
    "We will see more supervised shallow embedding algorithms here.\n",
    "\n",
    "## Label Propagation Algorithm\n",
    "Used to solve node classification task, the algorithm propagates the label of a given node to its neighbours or to nodes having high probability of being reached from that node.\n",
    "\n",
    "The only nonzero elements of the degree matrix are diagonal elements whose values represent the degree of the node represented by the row. Introduce transition matrix $L=$$D$<sup>$-1$</sup>=$A$ where l<sub>$ij$</sub>$\\in L$ is the probability of reaching node $v_j$ from $v_i$. Probability of reaching an end node given a start node. \n",
    "\n",
    "Can see the probability of nodes being assigned labels. So we can perform *n* iterations, at each iteration *t*, the algorithm will compute the solution for that iteration:\n",
    "$Y^t = LY$<sup>$t-1$</sup>\n",
    "And stops when a certain condition is met.\n",
    "\n",
    "However we can see the issues:\n",
    "-  Possible to assigned only to nodes a probability associated with a label\n",
    "-  The initial labels of values are different from the the one defined in $Y^0$ \n",
    "    -  Can solve by forcing labeled nodes to have initial class values instead of losing its own values\n",
    "    \n",
    "And the algorithm runs until we reach a certain number of iterations or hit a solution tolerance error. Here we may see error with fixing the value of Y0 for original, especially if there is a labelling error, that may propagate itself. So we change the algorithm to normalised Laplacian $L = D$<sup>$-1/2$</sup>$AD$<sup>$-1/2$</sup> and change our propagation agorithm to $Y^t = \\alpha L Y$<sup>t-1</sup>$ + (1-\\alpha)Y^0$ and stops when a certain conition is met. Here we have regulariser $\\alpha \\in [0,1]$ to weight influence of original solution at each iteration, imposing the \"quality\" of the original solution and its influence in the final solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph regularisation methods\n",
    "Topological information and relations between data points can be encoded and leveraged to build more robust classifiers. Using network information to constrain models and enforce smooth outputs within neighbouring nodes. This can also be used to regulate the learning phase to create more robust models that tend to generalise better to unseen examples. Both the label propagation and label spreading can be implemented as a cost function to be minimised with an added regularisation term.\n",
    "\n",
    "A loss function that depends on labeled and unlabelled samples, with the second (on unlablled) term acting as a regularising term that depends on the topological information of graph *G*. This can be powerful as a tool to regularise the training of neural networks. \n",
    "\n",
    "\n",
    "## Manifold Regularisation and semi-supervised embedding\n",
    "Manifold regularisation extends label propagation by parameterising the model function in reproducing kernel Hilbert space and using a supervised loss function, mean square error. So when training SVM or LSE, apply graph regularisation based on Laplacian matrix. Label propagation and label spreading can be seen as a special case of manifold regularisation. Besides, the algorithms can also be used in the case of no-label data. It can also be used for fully labeled datasets, or on unobserved samples making it an inductive model.\n",
    "\n",
    "**Inductive model**: Can be used on unobserved samples and does not require test samples to belong to input graph.\n",
    "\n",
    "**Manifold learning**: A shallow form of learning whereby the the parameterised function does not leverage on any form of intermediate embeddings\n",
    "\n",
    "**Semi-supervised embedding**: Extends concepts of graph regularisation to deeper architectures by imposing the constraint and smoothness of function on intermediate layers of network\n",
    "\n",
    "Depending on where the regularisation is imposed, we can have three different configurations:\n",
    "\n",
    "-  Regularisation can be applied to final output of network.\n",
    "-  Regularisation applied to inetermediate layers, regularising the embedding representation\n",
    "-  Regularisation applied to an auxiliary network that shares first k-1 layers; corresponds to training an unsupervised embedding network while simultaneously training a supervised network. Imposes derived regularisation of first k-1 layers constrained by unsupervised network as well and simultaneously promotes an embedding of the network nodes.\n",
    "\n",
    "We have loss functions that ensures embeddings of neighbouring nodes stay close. Non-neighbours are pulled apart to distance specified by threshold *m*. The best choice of the above depends on data. Be aware that embeddings in deeper layers are generally harder to be trained and require a careful tuning of learning rate and margins to be used. Also when using softmax (usually at output), hinge loss may not be appropriate or suited for log probabilities. In such a case, regularised embeddings and relative loss should instead be introduced at intermediate layers.\n",
    "\n",
    "## Neural Graph Learning\n",
    "Generalises previous formulations to make it possible to apply graph regularisation to any form of a NN. Can apply to any graph, natural or synthetic. We can also generate synthetic graphs with adversarial examples where samples are perturbed to maximise errors, allowing us to obtain models more robust against adversarially generated examples.\n",
    "\n",
    "NGL extends regularisation by augmenting the tuning parameters for graph regularisation in NN's, decomposing the contribution of labeled-labeled, labeled-unlabeled and unlabeled-unlabeled relations with parameters $\\alpha_1$, $\\alpha_2$ and $\\alpha_3$.\n",
    "\n",
    "Loosely speaking, NGL formulations can be seen as non-linear versions of label propagation and label spreading algorithms, or as a form of graph-regularised NN for which the manifold learning or semi-supervising embeddings can be obtained.\n",
    "\n",
    "Here we will work on the **Cora** dataset, a labeled dataset of 2,708 scientific papers in comp-sci classified into seven classes. Each paper represents a node connected to other nodes based on citations. In total there are 5,429 links in the network. Furthermore, each node is described by a 1,433-long vector of binary values (0 or1) that represent a dicohotomic **bag-of-words (BOW)** representation of the paper; a one-hot encoding algorithm indicating the presence/absence of a word in a given vocabulary made up of 1,433 terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download cora dataset\n",
    "from stellargraph import datasets\n",
    "dataset = datasets.Cora()\n",
    "dataset.download()\n",
    "# G is the citation network with network nodes, edges and features describing the BOW representation\n",
    "# labels is a pd series providing the mapping between paper ID and one of the classes\n",
    "G, labels = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# structure node features as dataframe\n",
    "adjMatrix = pd.DataFrame.sparse.from_spmatrix(\n",
    "    G.to_adjacency_matrix(),\n",
    "    index=G.nodes(),\n",
    "    columns=G.nodes()\n",
    ")\n",
    "# store node features as adjacency matrix\n",
    "features = pd.DataFrame(G.node_features(), index=G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2708, 2708), (2708, 1433))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjMatrix.shape, features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighbours(idx, adjMatrix, topn=5):\n",
    "    # helper fn to retrieve closest topn neighbours of a node\n",
    "    weights = adjMatrix.loc[idx]\n",
    "    neighbours = weights[weights>0].sort_values(ascending=False).head(topn)\n",
    "    return [(k, v) for k, v in neighbours.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataFrame' has no attribute 'frm_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-38baeaa23d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m }\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrm_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DataFrame' has no attribute 'frm_dict'"
     ]
    }
   ],
   "source": [
    "topn = 5\n",
    "label_index = {\n",
    "      'Case_Based': 0,\n",
    "      'Genetic_Algorithms': 1,\n",
    "      'Neural_Networks': 2,\n",
    "      'Probabilistic_Methods': 3,\n",
    "      'Reinforcement_Learning': 4,\n",
    "      'Rule_Learning': 5,\n",
    "      'Theory': 6,\n",
    "  }\n",
    "\n",
    "# merge information into a single dataframe\n",
    "dataset = {\n",
    "    index: {\n",
    "        'id': index,\n",
    "        'words': [float(x) for x in features.loc[index].values],\n",
    "        'label': label_index[label],\n",
    "        'neighbours': getNeighbours(index, adjMatrix, topn)\n",
    "    }\n",
    "    for index, label in labels.items()\n",
    "}\n",
    "df = pd.DataFrame.from_dict(dataset, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "      <th>neighbours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>31336</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(1129442, 1.0), (686532, 1.0), (10531, 1.0), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              words  label  \\\n",
       "31336  31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      2   \n",
       "\n",
       "                                              neighbours  \n",
       "31336  [(1129442, 1.0), (686532, 1.0), (10531, 1.0), ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_PREFIX=\"NL_nbr\"\n",
    "\n",
    "def getFeatureOrDefault(ith, row):\n",
    "    # define a function to retrieve and join the neighbourhood information\n",
    "    # to join preceeding dataframe with information from neighbourhood\n",
    "    try:\n",
    "        nodeId, value = row[\"neighbours\"][ith]\n",
    "        return {\n",
    "            f\"{GRAPH_PREFIX}_{ith}_weight\": value,\n",
    "            f\"{GRAPH_PREFIX}_{ith}_words\": df.loc[nodeId][\"words\"]\n",
    "        }\n",
    "    except:\n",
    "        # when neighboursare less than topn, set weight and one-hot encoding to 0\n",
    "        return {\n",
    "            f\"{GRAPH_PREFIX}_{ith}_weight\": 0.0,\n",
    "            f\"{GRAPH_PREFIX}_{ith}_words\": [float(x) for x in np.zeros(1433)]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighboursFeatures(row):\n",
    "    featureList = [getFeatureOrDefault(ith, row) for ith in range(topn)]\n",
    "    return pd.Series(\n",
    "        {k: v for feat in featureList for k, v in feat.items()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = df.apply(neighboursFeatures, axis=1)\n",
    "allFeatures = pd.concat([df, neighbours], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "      <th>neighbours</th>\n",
       "      <th>NL_nbr_0_weight</th>\n",
       "      <th>NL_nbr_0_words</th>\n",
       "      <th>NL_nbr_1_weight</th>\n",
       "      <th>NL_nbr_1_words</th>\n",
       "      <th>NL_nbr_2_weight</th>\n",
       "      <th>NL_nbr_2_words</th>\n",
       "      <th>NL_nbr_3_weight</th>\n",
       "      <th>NL_nbr_3_words</th>\n",
       "      <th>NL_nbr_4_weight</th>\n",
       "      <th>NL_nbr_4_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>31336</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[(1129442, 1.0), (686532, 1.0), (10531, 1.0), ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              words  label  \\\n",
       "31336  31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      2   \n",
       "\n",
       "                                              neighbours  NL_nbr_0_weight  \\\n",
       "31336  [(1129442, 1.0), (686532, 1.0), (10531, 1.0), ...              1.0   \n",
       "\n",
       "                                          NL_nbr_0_words  NL_nbr_1_weight  \\\n",
       "31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              1.0   \n",
       "\n",
       "                                          NL_nbr_1_words  NL_nbr_2_weight  \\\n",
       "31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              1.0   \n",
       "\n",
       "                                          NL_nbr_2_words  NL_nbr_3_weight  \\\n",
       "31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              1.0   \n",
       "\n",
       "                                          NL_nbr_3_words  NL_nbr_4_weight  \\\n",
       "31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...              1.0   \n",
       "\n",
       "                                          NL_nbr_4_words  \n",
       "31336  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFeatures.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# split to train and test dataset\n",
    "ratio = 0.2 # change amount of labeled vs unlabeled data pts\n",
    "\n",
    "n = int(np.round(len(labels) * ratio))\n",
    "labelled, unlabelled = model_selection.train_test_split(\n",
    "    allFeatures, train_size=n, test_size=None, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_base = {\n",
    "    \"words\": tf.constant([\n",
    "        tuple(x) for x in labelled[\"words\"].values\n",
    "    ]),\n",
    "    \"label\": tf.constant([\n",
    "        x for x in labelled[\"label\"].values\n",
    "    ])\n",
    "}\n",
    "train_neighbour_words = {\n",
    "    k: tf.constant([tuple(x) for x in labelled[k].values]) for k in neighbours if \"words\" in k\n",
    "}\n",
    "train_neighbour_weights = {\n",
    "    k: tf.constant([tuple(x) for x in labelled[k].values]) for k in neighbours if \"weigt\" in k\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all information in tfds\n",
    "trainSet = tf.data.Dataset.from_tensor_slices({\n",
    "    k:v for feature in [train_base, train_neighbour_words, train_neighbour_weights] for k, v in feature.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "validSet = tf.data.Dataset.from_tensor_slices({\n",
    "    \"words\": tf.constant([tuple(x) for x in unlabelled[\"words\"].values]),\n",
    "    \"label\": tf.constant([x for x in unlabelled[\"label\"].values])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(features):\n",
    "    labels=features.pop(\"label\")\n",
    "    return features, labels\n",
    "\n",
    "trainSet = trainSet.map(split)\n",
    "validSet = validSet.map(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_0_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_1_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_2_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_3_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_4_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>}\n",
      "tf.Tensor([2 2], shape=(2,), dtype=int32)\n",
      "{'words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_0_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_1_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_2_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_3_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, 'NL_nbr_4_words': <tf.Tensor: shape=(2, 1433), dtype=float32, numpy=\n",
      "array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>}\n",
      "tf.Tensor([2 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for features, labels in trainSet.batch(2).take(2):\n",
    "    print(features)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularySize = 1433\n",
    "\n",
    "inputs = tf.keras.Input(shape=(vocabularySize,), dtype='float32', name='words')\n",
    "cur_layer = inputs\n",
    "for num_units in [50, 50]:\n",
    "    cur_layer = tf.keras.layers.Dense(num_units, activation='relu')(cur_layer)\n",
    "    cur_layer = tf.keras.layers.Dropout(0.8)(cur_layer)\n",
    "outputs = tf.keras.layers.Dense(len(label_index), activation='softmax', name='label')(cur_layer)\n",
    "model = tf.keras.Model(inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "words (InputLayer)           [(None, 1433)]            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                71700     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "label (Dense)                (None, 7)                 357       \n",
      "=================================================================\n",
      "Total params: 74,607\n",
      "Trainable params: 74,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['NL_nbr_0_words', 'NL_nbr_1_words', 'NL_nbr_2_words', 'NL_nbr_3_words', 'NL_nbr_4_words'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 1.9835 - accuracy: 0.1797WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 1.9928 - accuracy: 0.1758WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0780s vs `on_train_batch_end` time: 0.1943s). Check your callbacks.\n",
      "5/5 [==============================] - 1s 275ms/step - loss: 2.0032 - accuracy: 0.1863 - val_loss: 1.9274 - val_accuracy: 0.1893\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.9923 - accuracy: 0.1974 - val_loss: 1.9169 - val_accuracy: 0.2405\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.9816 - accuracy: 0.1790 - val_loss: 1.9084 - val_accuracy: 0.2779\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 261ms/step - loss: 1.9954 - accuracy: 0.1863 - val_loss: 1.9019 - val_accuracy: 0.3056\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 1.9521 - accuracy: 0.2140 - val_loss: 1.8958 - val_accuracy: 0.3223\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 1.9240 - accuracy: 0.2103 - val_loss: 1.8905 - val_accuracy: 0.3273\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.9287 - accuracy: 0.1919 - val_loss: 1.8852 - val_accuracy: 0.3292\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 264ms/step - loss: 1.9193 - accuracy: 0.2417 - val_loss: 1.8799 - val_accuracy: 0.3278\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 1.8931 - accuracy: 0.2509 - val_loss: 1.8742 - val_accuracy: 0.3250\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.8869 - accuracy: 0.2232 - val_loss: 1.8681 - val_accuracy: 0.3232\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.8572 - accuracy: 0.2325 - val_loss: 1.8616 - val_accuracy: 0.3176\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.8754 - accuracy: 0.2399 - val_loss: 1.8556 - val_accuracy: 0.3144\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.8762 - accuracy: 0.2472 - val_loss: 1.8503 - val_accuracy: 0.3116\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 1.8686 - accuracy: 0.2399 - val_loss: 1.8454 - val_accuracy: 0.3098\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 1.8362 - accuracy: 0.2860 - val_loss: 1.8404 - val_accuracy: 0.3084\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 1.8327 - accuracy: 0.2694 - val_loss: 1.8344 - val_accuracy: 0.3079\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.7897 - accuracy: 0.2841 - val_loss: 1.8275 - val_accuracy: 0.3075\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.8069 - accuracy: 0.2804 - val_loss: 1.8195 - val_accuracy: 0.3079\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 1.7718 - accuracy: 0.2768 - val_loss: 1.8106 - val_accuracy: 0.3079\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 1.7903 - accuracy: 0.2878 - val_loss: 1.8028 - val_accuracy: 0.3079\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 1.7776 - accuracy: 0.2786 - val_loss: 1.7946 - val_accuracy: 0.3084\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 1.7520 - accuracy: 0.3007 - val_loss: 1.7854 - val_accuracy: 0.3075\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 263ms/step - loss: 1.7490 - accuracy: 0.3007 - val_loss: 1.7756 - val_accuracy: 0.3070\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 1.7170 - accuracy: 0.3063 - val_loss: 1.7660 - val_accuracy: 0.3070\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.7172 - accuracy: 0.3339 - val_loss: 1.7560 - val_accuracy: 0.3066\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.7161 - accuracy: 0.3173 - val_loss: 1.7466 - val_accuracy: 0.3084\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.7406 - accuracy: 0.2786 - val_loss: 1.7374 - val_accuracy: 0.3098\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 1.6903 - accuracy: 0.3044 - val_loss: 1.7274 - val_accuracy: 0.3102\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 191ms/step - loss: 1.6823 - accuracy: 0.3247 - val_loss: 1.7175 - val_accuracy: 0.3121\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.6829 - accuracy: 0.3192 - val_loss: 1.7071 - val_accuracy: 0.3149\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 1.6670 - accuracy: 0.3192 - val_loss: 1.6962 - val_accuracy: 0.3167\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 1.6402 - accuracy: 0.3247 - val_loss: 1.6845 - val_accuracy: 0.3199\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.6359 - accuracy: 0.3524 - val_loss: 1.6726 - val_accuracy: 0.3255\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 1.6183 - accuracy: 0.3524 - val_loss: 1.6611 - val_accuracy: 0.3292\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 1.6252 - accuracy: 0.3579 - val_loss: 1.6493 - val_accuracy: 0.3366\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 1.5524 - accuracy: 0.3487 - val_loss: 1.6372 - val_accuracy: 0.3403\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 1.5907 - accuracy: 0.3487 - val_loss: 1.6261 - val_accuracy: 0.3467\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 1.6039 - accuracy: 0.3321 - val_loss: 1.6158 - val_accuracy: 0.3495\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 1.5575 - accuracy: 0.3727 - val_loss: 1.6055 - val_accuracy: 0.3546\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 1.5652 - accuracy: 0.3321 - val_loss: 1.5954 - val_accuracy: 0.3620\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 1.5272 - accuracy: 0.3690 - val_loss: 1.5857 - val_accuracy: 0.3707\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.5553 - accuracy: 0.3598 - val_loss: 1.5767 - val_accuracy: 0.3763\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.5123 - accuracy: 0.3782 - val_loss: 1.5671 - val_accuracy: 0.3813\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 1.5349 - accuracy: 0.3745 - val_loss: 1.5576 - val_accuracy: 0.3887\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 2s 360ms/step - loss: 1.4801 - accuracy: 0.3819 - val_loss: 1.5488 - val_accuracy: 0.3952\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 1s 197ms/step - loss: 1.4842 - accuracy: 0.3985 - val_loss: 1.5391 - val_accuracy: 0.4012\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 1.4876 - accuracy: 0.3708 - val_loss: 1.5288 - val_accuracy: 0.4086\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 1.4629 - accuracy: 0.3764 - val_loss: 1.5192 - val_accuracy: 0.4178\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.4614 - accuracy: 0.3708 - val_loss: 1.5095 - val_accuracy: 0.4201\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 1.4154 - accuracy: 0.4096 - val_loss: 1.5006 - val_accuracy: 0.4261\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 1.4254 - accuracy: 0.3967 - val_loss: 1.4917 - val_accuracy: 0.4312\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.4620 - accuracy: 0.4022 - val_loss: 1.4830 - val_accuracy: 0.4358\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 1.4743 - accuracy: 0.3838 - val_loss: 1.4758 - val_accuracy: 0.4409\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 1.3560 - accuracy: 0.4133 - val_loss: 1.4679 - val_accuracy: 0.4501\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.3707 - accuracy: 0.4207 - val_loss: 1.4592 - val_accuracy: 0.4580\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 1.3858 - accuracy: 0.4373 - val_loss: 1.4501 - val_accuracy: 0.4635\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 1.4020 - accuracy: 0.3985 - val_loss: 1.4431 - val_accuracy: 0.4700\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 1.3529 - accuracy: 0.4391 - val_loss: 1.4346 - val_accuracy: 0.4765\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.4126 - accuracy: 0.4170 - val_loss: 1.4264 - val_accuracy: 0.4857\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 1.3734 - accuracy: 0.4188 - val_loss: 1.4193 - val_accuracy: 0.4885\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 1.3611 - accuracy: 0.4170 - val_loss: 1.4122 - val_accuracy: 0.4917\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 1.3179 - accuracy: 0.4354 - val_loss: 1.4046 - val_accuracy: 0.4954\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 1.3374 - accuracy: 0.4428 - val_loss: 1.3969 - val_accuracy: 0.5009\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 1.3057 - accuracy: 0.4576 - val_loss: 1.3894 - val_accuracy: 0.5060\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 1.2529 - accuracy: 0.4889 - val_loss: 1.3811 - val_accuracy: 0.5143\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.2544 - accuracy: 0.4982 - val_loss: 1.3722 - val_accuracy: 0.5217\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.2486 - accuracy: 0.4815 - val_loss: 1.3639 - val_accuracy: 0.5259\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 1.2765 - accuracy: 0.4613 - val_loss: 1.3560 - val_accuracy: 0.5314\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 1.2388 - accuracy: 0.4852 - val_loss: 1.3486 - val_accuracy: 0.5360\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.2255 - accuracy: 0.4779 - val_loss: 1.3407 - val_accuracy: 0.5411\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.2561 - accuracy: 0.4668 - val_loss: 1.3329 - val_accuracy: 0.5462\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 2s 319ms/step - loss: 1.2713 - accuracy: 0.4446 - val_loss: 1.3257 - val_accuracy: 0.5545\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 1.2256 - accuracy: 0.4889 - val_loss: 1.3189 - val_accuracy: 0.5591\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.2213 - accuracy: 0.4945 - val_loss: 1.3143 - val_accuracy: 0.5591\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.2030 - accuracy: 0.4668 - val_loss: 1.3103 - val_accuracy: 0.5609\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 1.2118 - accuracy: 0.4797 - val_loss: 1.3053 - val_accuracy: 0.5619\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 1.1885 - accuracy: 0.4963 - val_loss: 1.2998 - val_accuracy: 0.5674\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 1.1866 - accuracy: 0.5203 - val_loss: 1.2954 - val_accuracy: 0.5679\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 1.1917 - accuracy: 0.5240 - val_loss: 1.2921 - val_accuracy: 0.5679\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 1.1440 - accuracy: 0.5332 - val_loss: 1.2884 - val_accuracy: 0.5702\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 1.1461 - accuracy: 0.5295 - val_loss: 1.2853 - val_accuracy: 0.5720\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 1.1741 - accuracy: 0.5074 - val_loss: 1.2812 - val_accuracy: 0.5716\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 1.1767 - accuracy: 0.5092 - val_loss: 1.2752 - val_accuracy: 0.5748\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 1.1170 - accuracy: 0.5443 - val_loss: 1.2700 - val_accuracy: 0.5789\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.1140 - accuracy: 0.5424 - val_loss: 1.2654 - val_accuracy: 0.5813\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 1.1245 - accuracy: 0.5221 - val_loss: 1.2609 - val_accuracy: 0.5826\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 1.1134 - accuracy: 0.5554 - val_loss: 1.2565 - val_accuracy: 0.5868\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 1.0691 - accuracy: 0.5720 - val_loss: 1.2534 - val_accuracy: 0.5863\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 1.0799 - accuracy: 0.5590 - val_loss: 1.2518 - val_accuracy: 0.5863\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 1.0796 - accuracy: 0.5351 - val_loss: 1.2494 - val_accuracy: 0.5849\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 1.0768 - accuracy: 0.5406 - val_loss: 1.2473 - val_accuracy: 0.5849\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 1.0508 - accuracy: 0.5535 - val_loss: 1.2464 - val_accuracy: 0.5854\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 1.0678 - accuracy: 0.5683 - val_loss: 1.2461 - val_accuracy: 0.5854\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 281ms/step - loss: 1.0683 - accuracy: 0.5738 - val_loss: 1.2463 - val_accuracy: 0.5868\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 1.0423 - accuracy: 0.5812 - val_loss: 1.2426 - val_accuracy: 0.5886\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9876 - accuracy: 0.5793 - val_loss: 1.2389 - val_accuracy: 0.5896\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 1.0285 - accuracy: 0.5664 - val_loss: 1.2370 - val_accuracy: 0.5900\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 1.0351 - accuracy: 0.5683 - val_loss: 1.2355 - val_accuracy: 0.5905\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.9898 - accuracy: 0.5701 - val_loss: 1.2350 - val_accuracy: 0.5905\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 204ms/step - loss: 1.0039 - accuracy: 0.5775 - val_loss: 1.2337 - val_accuracy: 0.5900\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.9986 - accuracy: 0.6052 - val_loss: 1.2333 - val_accuracy: 0.5905\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.9836 - accuracy: 0.5996 - val_loss: 1.2301 - val_accuracy: 0.5933\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.9953 - accuracy: 0.5941 - val_loss: 1.2280 - val_accuracy: 0.5937\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 1.0050 - accuracy: 0.5775 - val_loss: 1.2285 - val_accuracy: 0.5942\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 257ms/step - loss: 0.9738 - accuracy: 0.5904 - val_loss: 1.2278 - val_accuracy: 0.5951\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.9987 - accuracy: 0.5720 - val_loss: 1.2221 - val_accuracy: 0.6011\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.9821 - accuracy: 0.5793 - val_loss: 1.2173 - val_accuracy: 0.6034\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.9483 - accuracy: 0.6089 - val_loss: 1.2169 - val_accuracy: 0.6062\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.9485 - accuracy: 0.6144 - val_loss: 1.2185 - val_accuracy: 0.6057\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.9623 - accuracy: 0.6292 - val_loss: 1.2212 - val_accuracy: 0.6053\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9100 - accuracy: 0.6162 - val_loss: 1.2274 - val_accuracy: 0.6057\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.9573 - accuracy: 0.6107 - val_loss: 1.2321 - val_accuracy: 0.6076\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.9444 - accuracy: 0.5959 - val_loss: 1.2318 - val_accuracy: 0.6085\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.9226 - accuracy: 0.6107 - val_loss: 1.2297 - val_accuracy: 0.6085\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 197ms/step - loss: 1.0094 - accuracy: 0.5683 - val_loss: 1.2322 - val_accuracy: 0.6076\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.9110 - accuracy: 0.6236 - val_loss: 1.2346 - val_accuracy: 0.6071\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 154ms/step - loss: 0.9132 - accuracy: 0.6144 - val_loss: 1.2377 - val_accuracy: 0.6057\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.8697 - accuracy: 0.6421 - val_loss: 1.2410 - val_accuracy: 0.6062\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 2s 341ms/step - loss: 0.9298 - accuracy: 0.6181 - val_loss: 1.2421 - val_accuracy: 0.6071\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.9196 - accuracy: 0.6107 - val_loss: 1.2430 - val_accuracy: 0.6071\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.8982 - accuracy: 0.6531 - val_loss: 1.2428 - val_accuracy: 0.6103\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.8944 - accuracy: 0.6550 - val_loss: 1.2432 - val_accuracy: 0.6108\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.9592 - accuracy: 0.6255 - val_loss: 1.2466 - val_accuracy: 0.6103\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.9018 - accuracy: 0.6199 - val_loss: 1.2516 - val_accuracy: 0.6090\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.8747 - accuracy: 0.6642 - val_loss: 1.2548 - val_accuracy: 0.6099\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.8520 - accuracy: 0.6882 - val_loss: 1.2577 - val_accuracy: 0.6103\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 2s 333ms/step - loss: 0.8755 - accuracy: 0.6605 - val_loss: 1.2608 - val_accuracy: 0.6108\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.8687 - accuracy: 0.6697 - val_loss: 1.2633 - val_accuracy: 0.6113\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.8193 - accuracy: 0.6808 - val_loss: 1.2650 - val_accuracy: 0.6131\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.8713 - accuracy: 0.6605 - val_loss: 1.2668 - val_accuracy: 0.6136\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.8632 - accuracy: 0.6937 - val_loss: 1.2643 - val_accuracy: 0.6163\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.8555 - accuracy: 0.6716 - val_loss: 1.2616 - val_accuracy: 0.6182\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.8150 - accuracy: 0.7011 - val_loss: 1.2649 - val_accuracy: 0.6177\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.8524 - accuracy: 0.7030 - val_loss: 1.2645 - val_accuracy: 0.6196\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.8599 - accuracy: 0.6679 - val_loss: 1.2617 - val_accuracy: 0.6200\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 0.8475 - accuracy: 0.6587 - val_loss: 1.2562 - val_accuracy: 0.6223\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.8179 - accuracy: 0.6900 - val_loss: 1.2517 - val_accuracy: 0.6256\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 295ms/step - loss: 0.9012 - accuracy: 0.6494 - val_loss: 1.2472 - val_accuracy: 0.6265\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.8279 - accuracy: 0.7140 - val_loss: 1.2500 - val_accuracy: 0.6293\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.7820 - accuracy: 0.7251 - val_loss: 1.2571 - val_accuracy: 0.6293\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 0.8399 - accuracy: 0.6919 - val_loss: 1.2653 - val_accuracy: 0.6283\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.8407 - accuracy: 0.6863 - val_loss: 1.2726 - val_accuracy: 0.6274\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.8342 - accuracy: 0.7030 - val_loss: 1.2804 - val_accuracy: 0.6270\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.7965 - accuracy: 0.7325 - val_loss: 1.2851 - val_accuracy: 0.6265\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.8120 - accuracy: 0.6771 - val_loss: 1.2879 - val_accuracy: 0.6256\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.8102 - accuracy: 0.6956 - val_loss: 1.2874 - val_accuracy: 0.6270\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.8259 - accuracy: 0.7177 - val_loss: 1.2882 - val_accuracy: 0.6274\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.7795 - accuracy: 0.7103 - val_loss: 1.2894 - val_accuracy: 0.6288\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 0.7785 - accuracy: 0.7103 - val_loss: 1.2888 - val_accuracy: 0.6274\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 0.8539 - accuracy: 0.7066 - val_loss: 1.2899 - val_accuracy: 0.6288\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 260ms/step - loss: 0.8140 - accuracy: 0.7048 - val_loss: 1.2942 - val_accuracy: 0.6288\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.8028 - accuracy: 0.6974 - val_loss: 1.3003 - val_accuracy: 0.6279\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.7787 - accuracy: 0.7251 - val_loss: 1.3070 - val_accuracy: 0.6283\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.8246 - accuracy: 0.7011 - val_loss: 1.3139 - val_accuracy: 0.6283\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 157ms/step - loss: 0.7540 - accuracy: 0.7177 - val_loss: 1.3198 - val_accuracy: 0.6293\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.7858 - accuracy: 0.7177 - val_loss: 1.3258 - val_accuracy: 0.6297\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.7646 - accuracy: 0.7196 - val_loss: 1.3310 - val_accuracy: 0.6288\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.7948 - accuracy: 0.7196 - val_loss: 1.3363 - val_accuracy: 0.6283\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 0.6969 - accuracy: 0.7565 - val_loss: 1.3411 - val_accuracy: 0.6270\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.7417 - accuracy: 0.7325 - val_loss: 1.3470 - val_accuracy: 0.6251\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.7599 - accuracy: 0.7159 - val_loss: 1.3542 - val_accuracy: 0.6242\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.7063 - accuracy: 0.7399 - val_loss: 1.3628 - val_accuracy: 0.6237\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.7328 - accuracy: 0.7435 - val_loss: 1.3662 - val_accuracy: 0.6233\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.8065 - accuracy: 0.6993 - val_loss: 1.3739 - val_accuracy: 0.6214\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.7476 - accuracy: 0.7232 - val_loss: 1.3835 - val_accuracy: 0.6214\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.7529 - accuracy: 0.7232 - val_loss: 1.3891 - val_accuracy: 0.6210\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.7271 - accuracy: 0.7325 - val_loss: 1.3887 - val_accuracy: 0.6214\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.7299 - accuracy: 0.7435 - val_loss: 1.3844 - val_accuracy: 0.6223\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.7084 - accuracy: 0.7454 - val_loss: 1.3825 - val_accuracy: 0.6242\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.7868 - accuracy: 0.6956 - val_loss: 1.3798 - val_accuracy: 0.6247\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.8032 - accuracy: 0.6900 - val_loss: 1.3767 - val_accuracy: 0.6237\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.7529 - accuracy: 0.7232 - val_loss: 1.3795 - val_accuracy: 0.6242\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 2s 338ms/step - loss: 0.7084 - accuracy: 0.7491 - val_loss: 1.3848 - val_accuracy: 0.6242\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.7502 - accuracy: 0.7103 - val_loss: 1.3904 - val_accuracy: 0.6260\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.7639 - accuracy: 0.7177 - val_loss: 1.3957 - val_accuracy: 0.6270\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.7167 - accuracy: 0.7380 - val_loss: 1.4019 - val_accuracy: 0.6270\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.7686 - accuracy: 0.6974 - val_loss: 1.4079 - val_accuracy: 0.6279\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.7380 - accuracy: 0.7362 - val_loss: 1.4161 - val_accuracy: 0.6270\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.6778 - accuracy: 0.7638 - val_loss: 1.4255 - val_accuracy: 0.6265\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 213ms/step - loss: 0.6972 - accuracy: 0.7565 - val_loss: 1.4399 - val_accuracy: 0.6242\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6871 - accuracy: 0.7435 - val_loss: 1.4512 - val_accuracy: 0.6228\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.6929 - accuracy: 0.7509 - val_loss: 1.4548 - val_accuracy: 0.6233\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.7276 - accuracy: 0.7343 - val_loss: 1.4478 - val_accuracy: 0.6256\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.7216 - accuracy: 0.7306 - val_loss: 1.4374 - val_accuracy: 0.6283\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 0.6649 - accuracy: 0.7657 - val_loss: 1.4283 - val_accuracy: 0.6288\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.6929 - accuracy: 0.7583 - val_loss: 1.4271 - val_accuracy: 0.6325\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 0.7029 - accuracy: 0.7251 - val_loss: 1.4306 - val_accuracy: 0.6334\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6937 - accuracy: 0.7491 - val_loss: 1.4388 - val_accuracy: 0.6311\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.7261 - accuracy: 0.7325 - val_loss: 1.4464 - val_accuracy: 0.6302\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 204ms/step - loss: 0.7844 - accuracy: 0.6956 - val_loss: 1.4456 - val_accuracy: 0.6297\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.7772 - accuracy: 0.6937 - val_loss: 1.4445 - val_accuracy: 0.6307\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6257 - accuracy: 0.7565 - val_loss: 1.4480 - val_accuracy: 0.6320\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.7472 - accuracy: 0.7122 - val_loss: 1.4505 - val_accuracy: 0.6330\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.7196 - accuracy: 0.7232 - val_loss: 1.4560 - val_accuracy: 0.6330\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 175ms/step - loss: 0.6740 - accuracy: 0.7509 - val_loss: 1.4603 - val_accuracy: 0.6316\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.6849 - accuracy: 0.7491 - val_loss: 1.4675 - val_accuracy: 0.6311\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 171ms/step - loss: 0.6766 - accuracy: 0.7288 - val_loss: 1.4749 - val_accuracy: 0.6293\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.7027 - accuracy: 0.7454 - val_loss: 1.4883 - val_accuracy: 0.6279\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.6899 - accuracy: 0.7399 - val_loss: 1.5025 - val_accuracy: 0.6256\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.6265 - accuracy: 0.7620 - val_loss: 1.5119 - val_accuracy: 0.6251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa818e133d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "model.fit(\n",
    "    trainSet.batch(128), epochs=200, verbose=1,\n",
    "    validation_data=validSet.batch(128),\n",
    "    callbacks=[TensorBoard(log_dir='/tmp/base')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a graph regularised version\n",
    "import neural_structured_learning as nsl\n",
    "graph_reg_config=nsl.configs.make_graph_reg_config(\n",
    "    max_neighbors=2, # num neighbours to compute regularisation loss for each node\n",
    "    multiplier=0.1, # coefficients that tune importance of regularisation loss\n",
    "    distance_type=nsl.configs.DistanceType.L2, # pairwise distance d\n",
    "    sum_over_axis=-1 # whether weighted average sum should be calculated WRT features or to samples\n",
    ")\n",
    "graph_reg = nsl.keras.GraphRegularization(model, graph_reg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2/5 [===========>..................] - ETA: 0s - loss: 0.7066 - accuracy: 0.7148WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0095s vs `on_train_batch_end` time: 0.2926s). Check your callbacks.\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.7019 - accuracy: 0.7196 - val_loss: 1.5807 - val_accuracy: 0.6233\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 1s 283ms/step - loss: 0.6614 - accuracy: 0.7546 - val_loss: 1.6281 - val_accuracy: 0.6210\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6816 - accuracy: 0.7454 - val_loss: 1.6518 - val_accuracy: 0.6200\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.6744 - accuracy: 0.7362 - val_loss: 1.6755 - val_accuracy: 0.6214\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.7302 - accuracy: 0.7343 - val_loss: 1.6836 - val_accuracy: 0.6237\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.6781 - accuracy: 0.7509 - val_loss: 1.6853 - val_accuracy: 0.6260\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.6915 - accuracy: 0.7417 - val_loss: 1.6887 - val_accuracy: 0.6265\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.6648 - accuracy: 0.7565 - val_loss: 1.6944 - val_accuracy: 0.6256\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.6946 - accuracy: 0.7435 - val_loss: 1.6973 - val_accuracy: 0.6270\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.6615 - accuracy: 0.7546 - val_loss: 1.7089 - val_accuracy: 0.6251\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6603 - accuracy: 0.7491 - val_loss: 1.7250 - val_accuracy: 0.6247\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.6712 - accuracy: 0.7472 - val_loss: 1.7374 - val_accuracy: 0.6228\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.6355 - accuracy: 0.7528 - val_loss: 1.7389 - val_accuracy: 0.6237\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.6163 - accuracy: 0.7731 - val_loss: 1.7370 - val_accuracy: 0.6233\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 1s 297ms/step - loss: 0.6255 - accuracy: 0.7657 - val_loss: 1.7423 - val_accuracy: 0.6233\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 0.6771 - accuracy: 0.7583 - val_loss: 1.7446 - val_accuracy: 0.6233\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.6413 - accuracy: 0.7583 - val_loss: 1.7527 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.6332 - accuracy: 0.7638 - val_loss: 1.7638 - val_accuracy: 0.6265\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6297 - accuracy: 0.7712 - val_loss: 1.7815 - val_accuracy: 0.6242\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.6021 - accuracy: 0.7657 - val_loss: 1.7932 - val_accuracy: 0.6251\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 2s 300ms/step - loss: 0.6694 - accuracy: 0.7472 - val_loss: 1.7984 - val_accuracy: 0.6251\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.6201 - accuracy: 0.7694 - val_loss: 1.8075 - val_accuracy: 0.6256\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.6294 - accuracy: 0.7694 - val_loss: 1.8110 - val_accuracy: 0.6256\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.6092 - accuracy: 0.7768 - val_loss: 1.8148 - val_accuracy: 0.6265\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.6530 - accuracy: 0.7583 - val_loss: 1.8136 - val_accuracy: 0.6265\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 1s 172ms/step - loss: 0.6666 - accuracy: 0.7472 - val_loss: 1.8140 - val_accuracy: 0.6270\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.6268 - accuracy: 0.7620 - val_loss: 1.8195 - val_accuracy: 0.6260\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 1s 232ms/step - loss: 0.6504 - accuracy: 0.7472 - val_loss: 1.8266 - val_accuracy: 0.6270\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 1s 174ms/step - loss: 0.6312 - accuracy: 0.7528 - val_loss: 1.8388 - val_accuracy: 0.6274\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.5963 - accuracy: 0.7915 - val_loss: 1.8629 - val_accuracy: 0.6274\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.6617 - accuracy: 0.7528 - val_loss: 1.8866 - val_accuracy: 0.6251\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.6033 - accuracy: 0.7731 - val_loss: 1.9143 - val_accuracy: 0.6237\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6611 - accuracy: 0.7472 - val_loss: 1.9338 - val_accuracy: 0.6219\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5732 - accuracy: 0.7934 - val_loss: 1.9500 - val_accuracy: 0.6210\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.5780 - accuracy: 0.7786 - val_loss: 1.9536 - val_accuracy: 0.6219\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 1s 143ms/step - loss: 0.5940 - accuracy: 0.7841 - val_loss: 1.9665 - val_accuracy: 0.6214\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 2s 301ms/step - loss: 0.5934 - accuracy: 0.7749 - val_loss: 1.9741 - val_accuracy: 0.6228\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6209 - accuracy: 0.7583 - val_loss: 1.9693 - val_accuracy: 0.6242\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.6136 - accuracy: 0.7583 - val_loss: 1.9617 - val_accuracy: 0.6256\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.6026 - accuracy: 0.7786 - val_loss: 1.9665 - val_accuracy: 0.6265\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.5730 - accuracy: 0.7786 - val_loss: 1.9702 - val_accuracy: 0.6260\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.6541 - accuracy: 0.7435 - val_loss: 1.9780 - val_accuracy: 0.6270\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6254 - accuracy: 0.7601 - val_loss: 1.9899 - val_accuracy: 0.6260\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.6215 - accuracy: 0.7491 - val_loss: 2.0021 - val_accuracy: 0.6270\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.6570 - accuracy: 0.7362 - val_loss: 2.0124 - val_accuracy: 0.6260\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 2s 302ms/step - loss: 0.6546 - accuracy: 0.7509 - val_loss: 2.0154 - val_accuracy: 0.6260\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.5992 - accuracy: 0.7694 - val_loss: 2.0138 - val_accuracy: 0.6265\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 1s 212ms/step - loss: 0.6238 - accuracy: 0.7491 - val_loss: 2.0116 - val_accuracy: 0.6274\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.6005 - accuracy: 0.7712 - val_loss: 2.0119 - val_accuracy: 0.6283\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.6298 - accuracy: 0.7509 - val_loss: 2.0113 - val_accuracy: 0.6293\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.6146 - accuracy: 0.7731 - val_loss: 2.0125 - val_accuracy: 0.6293\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 0.6645 - accuracy: 0.7399 - val_loss: 2.0178 - val_accuracy: 0.6288\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.5744 - accuracy: 0.7878 - val_loss: 2.0303 - val_accuracy: 0.6302\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5696 - accuracy: 0.7804 - val_loss: 2.0456 - val_accuracy: 0.6307\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.5716 - accuracy: 0.7878 - val_loss: 2.0642 - val_accuracy: 0.6311\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 1s 177ms/step - loss: 0.5566 - accuracy: 0.7970 - val_loss: 2.0799 - val_accuracy: 0.6307\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.6303 - accuracy: 0.7583 - val_loss: 2.0890 - val_accuracy: 0.6307\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.6409 - accuracy: 0.7491 - val_loss: 2.0981 - val_accuracy: 0.6302\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.6235 - accuracy: 0.7509 - val_loss: 2.1132 - val_accuracy: 0.6293\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5587 - accuracy: 0.7860 - val_loss: 2.1273 - val_accuracy: 0.6297\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 0.6199 - accuracy: 0.7546 - val_loss: 2.1477 - val_accuracy: 0.6279\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.6143 - accuracy: 0.7786 - val_loss: 2.1658 - val_accuracy: 0.6279\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.5707 - accuracy: 0.7768 - val_loss: 2.1820 - val_accuracy: 0.6293\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.5846 - accuracy: 0.7768 - val_loss: 2.1983 - val_accuracy: 0.6279\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.5884 - accuracy: 0.7675 - val_loss: 2.2072 - val_accuracy: 0.6279\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.6196 - accuracy: 0.7565 - val_loss: 2.2087 - val_accuracy: 0.6297\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.6193 - accuracy: 0.7362 - val_loss: 2.2057 - val_accuracy: 0.6297\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.5757 - accuracy: 0.7804 - val_loss: 2.2107 - val_accuracy: 0.6288\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5576 - accuracy: 0.7804 - val_loss: 2.2164 - val_accuracy: 0.6283\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.6004 - accuracy: 0.7731 - val_loss: 2.2179 - val_accuracy: 0.6288\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.6111 - accuracy: 0.7731 - val_loss: 2.2250 - val_accuracy: 0.6270\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5856 - accuracy: 0.7768 - val_loss: 2.2222 - val_accuracy: 0.6270\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 0.6007 - accuracy: 0.7731 - val_loss: 2.2154 - val_accuracy: 0.6297\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.5962 - accuracy: 0.7694 - val_loss: 2.2131 - val_accuracy: 0.6311\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.5939 - accuracy: 0.7823 - val_loss: 2.2251 - val_accuracy: 0.6311\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.6443 - accuracy: 0.7454 - val_loss: 2.2381 - val_accuracy: 0.6297\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5847 - accuracy: 0.7786 - val_loss: 2.2505 - val_accuracy: 0.6274\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.5854 - accuracy: 0.7768 - val_loss: 2.2626 - val_accuracy: 0.6279\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 1s 283ms/step - loss: 0.6285 - accuracy: 0.7601 - val_loss: 2.2676 - val_accuracy: 0.6302\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 2s 321ms/step - loss: 0.5851 - accuracy: 0.7638 - val_loss: 2.2764 - val_accuracy: 0.6288\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 1s 255ms/step - loss: 0.5637 - accuracy: 0.7860 - val_loss: 2.2822 - val_accuracy: 0.6283\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.5510 - accuracy: 0.7860 - val_loss: 2.2837 - val_accuracy: 0.6283\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.5891 - accuracy: 0.7878 - val_loss: 2.2904 - val_accuracy: 0.6279\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.6019 - accuracy: 0.7694 - val_loss: 2.2959 - val_accuracy: 0.6274\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.5907 - accuracy: 0.7694 - val_loss: 2.3045 - val_accuracy: 0.6274\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.6090 - accuracy: 0.7509 - val_loss: 2.3205 - val_accuracy: 0.6283\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.5641 - accuracy: 0.7952 - val_loss: 2.3333 - val_accuracy: 0.6279\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 2s 332ms/step - loss: 0.5847 - accuracy: 0.7601 - val_loss: 2.3485 - val_accuracy: 0.6274\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.6119 - accuracy: 0.7601 - val_loss: 2.3627 - val_accuracy: 0.6270\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.6100 - accuracy: 0.7601 - val_loss: 2.3691 - val_accuracy: 0.6260\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.6127 - accuracy: 0.7509 - val_loss: 2.3768 - val_accuracy: 0.6270\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.5398 - accuracy: 0.7970 - val_loss: 2.3877 - val_accuracy: 0.6270\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5641 - accuracy: 0.7934 - val_loss: 2.4031 - val_accuracy: 0.6270\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.5480 - accuracy: 0.7786 - val_loss: 2.4022 - val_accuracy: 0.6256\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.6368 - accuracy: 0.7435 - val_loss: 2.3994 - val_accuracy: 0.6265\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5266 - accuracy: 0.8137 - val_loss: 2.3979 - val_accuracy: 0.6256\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.5756 - accuracy: 0.7712 - val_loss: 2.4087 - val_accuracy: 0.6247\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.5572 - accuracy: 0.7878 - val_loss: 2.4202 - val_accuracy: 0.6233\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5744 - accuracy: 0.7823 - val_loss: 2.4161 - val_accuracy: 0.6233\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 1s 152ms/step - loss: 0.5207 - accuracy: 0.8118 - val_loss: 2.4117 - val_accuracy: 0.6242\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 1s 218ms/step - loss: 0.5231 - accuracy: 0.8007 - val_loss: 2.4129 - val_accuracy: 0.6242\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.5888 - accuracy: 0.7749 - val_loss: 2.4139 - val_accuracy: 0.6251\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.5802 - accuracy: 0.7768 - val_loss: 2.4177 - val_accuracy: 0.6274\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.5401 - accuracy: 0.7841 - val_loss: 2.4256 - val_accuracy: 0.6279\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.5211 - accuracy: 0.8026 - val_loss: 2.4395 - val_accuracy: 0.6283\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.5920 - accuracy: 0.7675 - val_loss: 2.4553 - val_accuracy: 0.6288\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5236 - accuracy: 0.7989 - val_loss: 2.4761 - val_accuracy: 0.6283\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 0.5840 - accuracy: 0.7823 - val_loss: 2.4938 - val_accuracy: 0.6274\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.5210 - accuracy: 0.7970 - val_loss: 2.5108 - val_accuracy: 0.6274\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.5715 - accuracy: 0.7731 - val_loss: 2.5263 - val_accuracy: 0.6270\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 1s 156ms/step - loss: 0.5736 - accuracy: 0.7823 - val_loss: 2.5522 - val_accuracy: 0.6274\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.5627 - accuracy: 0.7841 - val_loss: 2.5642 - val_accuracy: 0.6279\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 1s 251ms/step - loss: 0.5689 - accuracy: 0.7749 - val_loss: 2.5716 - val_accuracy: 0.6283\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.5747 - accuracy: 0.7712 - val_loss: 2.5770 - val_accuracy: 0.6274\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.5794 - accuracy: 0.7768 - val_loss: 2.5894 - val_accuracy: 0.6270\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.5437 - accuracy: 0.7860 - val_loss: 2.5949 - val_accuracy: 0.6270\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.5481 - accuracy: 0.7897 - val_loss: 2.5952 - val_accuracy: 0.6265\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.5491 - accuracy: 0.7860 - val_loss: 2.6068 - val_accuracy: 0.6256\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 1s 142ms/step - loss: 0.5505 - accuracy: 0.7897 - val_loss: 2.6270 - val_accuracy: 0.6256\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 1s 160ms/step - loss: 0.4794 - accuracy: 0.8007 - val_loss: 2.6482 - val_accuracy: 0.6242\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.5383 - accuracy: 0.7952 - val_loss: 2.6587 - val_accuracy: 0.6242\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 1s 184ms/step - loss: 0.5007 - accuracy: 0.7952 - val_loss: 2.6489 - val_accuracy: 0.6265\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.5667 - accuracy: 0.7731 - val_loss: 2.6260 - val_accuracy: 0.6265\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.5504 - accuracy: 0.7841 - val_loss: 2.6184 - val_accuracy: 0.6270\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5147 - accuracy: 0.8007 - val_loss: 2.6211 - val_accuracy: 0.6274\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 1s 200ms/step - loss: 0.5274 - accuracy: 0.7804 - val_loss: 2.6337 - val_accuracy: 0.6279\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.6027 - accuracy: 0.7638 - val_loss: 2.6543 - val_accuracy: 0.6274\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.5762 - accuracy: 0.7694 - val_loss: 2.6737 - val_accuracy: 0.6265\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5390 - accuracy: 0.8007 - val_loss: 2.6879 - val_accuracy: 0.6260\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 1s 164ms/step - loss: 0.5370 - accuracy: 0.7860 - val_loss: 2.6920 - val_accuracy: 0.6260\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5595 - accuracy: 0.7860 - val_loss: 2.6943 - val_accuracy: 0.6270\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5505 - accuracy: 0.7915 - val_loss: 2.7014 - val_accuracy: 0.6265\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5514 - accuracy: 0.7970 - val_loss: 2.7195 - val_accuracy: 0.6270\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.5623 - accuracy: 0.7823 - val_loss: 2.7378 - val_accuracy: 0.6274\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5846 - accuracy: 0.7675 - val_loss: 2.7546 - val_accuracy: 0.6279\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.5050 - accuracy: 0.8007 - val_loss: 2.7677 - val_accuracy: 0.6293\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.5346 - accuracy: 0.7934 - val_loss: 2.7738 - val_accuracy: 0.6307\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.5173 - accuracy: 0.7989 - val_loss: 2.7934 - val_accuracy: 0.6302\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.5143 - accuracy: 0.7989 - val_loss: 2.8184 - val_accuracy: 0.6270\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 1s 283ms/step - loss: 0.5446 - accuracy: 0.7823 - val_loss: 2.8382 - val_accuracy: 0.6251\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5264 - accuracy: 0.7878 - val_loss: 2.8498 - val_accuracy: 0.6233\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 1s 299ms/step - loss: 0.5668 - accuracy: 0.7841 - val_loss: 2.8480 - val_accuracy: 0.6251\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.5138 - accuracy: 0.8044 - val_loss: 2.8299 - val_accuracy: 0.6242\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.5628 - accuracy: 0.7731 - val_loss: 2.8196 - val_accuracy: 0.6242\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.5553 - accuracy: 0.7897 - val_loss: 2.8219 - val_accuracy: 0.6233\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 1s 176ms/step - loss: 0.5491 - accuracy: 0.7749 - val_loss: 2.8365 - val_accuracy: 0.6237\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.5401 - accuracy: 0.7804 - val_loss: 2.8446 - val_accuracy: 0.6223\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.5518 - accuracy: 0.7952 - val_loss: 2.8410 - val_accuracy: 0.6223\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.5235 - accuracy: 0.7934 - val_loss: 2.8488 - val_accuracy: 0.6233\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.5501 - accuracy: 0.7749 - val_loss: 2.8583 - val_accuracy: 0.6237\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.5470 - accuracy: 0.7823 - val_loss: 2.8645 - val_accuracy: 0.6237\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5117 - accuracy: 0.8007 - val_loss: 2.8802 - val_accuracy: 0.6219\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 1s 198ms/step - loss: 0.5961 - accuracy: 0.7638 - val_loss: 2.8816 - val_accuracy: 0.6228\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 1s 282ms/step - loss: 0.5414 - accuracy: 0.7915 - val_loss: 2.8735 - val_accuracy: 0.6237\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 1s 203ms/step - loss: 0.4584 - accuracy: 0.8247 - val_loss: 2.8578 - val_accuracy: 0.6251\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 1s 179ms/step - loss: 0.4705 - accuracy: 0.8081 - val_loss: 2.8543 - val_accuracy: 0.6251\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 1s 180ms/step - loss: 0.5698 - accuracy: 0.7675 - val_loss: 2.8598 - val_accuracy: 0.6242\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 1s 259ms/step - loss: 0.5263 - accuracy: 0.7915 - val_loss: 2.8546 - val_accuracy: 0.6251\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.4978 - accuracy: 0.7952 - val_loss: 2.8516 - val_accuracy: 0.6251\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 1s 192ms/step - loss: 0.5441 - accuracy: 0.7823 - val_loss: 2.8607 - val_accuracy: 0.6251\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.5335 - accuracy: 0.7970 - val_loss: 2.8753 - val_accuracy: 0.6233\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 1s 162ms/step - loss: 0.4908 - accuracy: 0.8026 - val_loss: 2.8869 - val_accuracy: 0.6233\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.5157 - accuracy: 0.8007 - val_loss: 2.9061 - val_accuracy: 0.6228\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.4904 - accuracy: 0.8026 - val_loss: 2.9283 - val_accuracy: 0.6237\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 1s 219ms/step - loss: 0.5262 - accuracy: 0.7897 - val_loss: 2.9343 - val_accuracy: 0.6237\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.5490 - accuracy: 0.7860 - val_loss: 2.9491 - val_accuracy: 0.6242\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.4902 - accuracy: 0.8100 - val_loss: 2.9803 - val_accuracy: 0.6242\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 2s 319ms/step - loss: 0.5681 - accuracy: 0.7749 - val_loss: 2.9983 - val_accuracy: 0.6237\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.5089 - accuracy: 0.8026 - val_loss: 3.0199 - val_accuracy: 0.6228\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.4915 - accuracy: 0.8118 - val_loss: 3.0381 - val_accuracy: 0.6214\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 1s 158ms/step - loss: 0.5037 - accuracy: 0.7934 - val_loss: 3.0446 - val_accuracy: 0.6223\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.4918 - accuracy: 0.8118 - val_loss: 3.0517 - val_accuracy: 0.6233\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.5047 - accuracy: 0.8044 - val_loss: 3.0661 - val_accuracy: 0.6219\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.5209 - accuracy: 0.7897 - val_loss: 3.0770 - val_accuracy: 0.6205\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.4907 - accuracy: 0.8118 - val_loss: 3.0808 - val_accuracy: 0.6200\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 1s 173ms/step - loss: 0.5926 - accuracy: 0.7768 - val_loss: 3.0936 - val_accuracy: 0.6219\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.5369 - accuracy: 0.7823 - val_loss: 3.1031 - val_accuracy: 0.6205\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.5327 - accuracy: 0.7823 - val_loss: 3.1045 - val_accuracy: 0.6205\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.5234 - accuracy: 0.8044 - val_loss: 3.0819 - val_accuracy: 0.6205\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 1s 205ms/step - loss: 0.5709 - accuracy: 0.7657 - val_loss: 3.0607 - val_accuracy: 0.6233\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 1s 159ms/step - loss: 0.4771 - accuracy: 0.8063 - val_loss: 3.0425 - val_accuracy: 0.6237\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 1s 194ms/step - loss: 0.5420 - accuracy: 0.7860 - val_loss: 3.0186 - val_accuracy: 0.6251\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.4873 - accuracy: 0.8063 - val_loss: 3.0030 - val_accuracy: 0.6260\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 1s 161ms/step - loss: 0.5236 - accuracy: 0.7878 - val_loss: 3.0006 - val_accuracy: 0.6260\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 1s 199ms/step - loss: 0.5737 - accuracy: 0.7675 - val_loss: 3.0011 - val_accuracy: 0.6256\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.5420 - accuracy: 0.7841 - val_loss: 3.0039 - val_accuracy: 0.6251\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 1s 144ms/step - loss: 0.5286 - accuracy: 0.7897 - val_loss: 3.0159 - val_accuracy: 0.6237\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 1s 202ms/step - loss: 0.4839 - accuracy: 0.8155 - val_loss: 3.0256 - val_accuracy: 0.6233\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.5543 - accuracy: 0.7823 - val_loss: 3.0338 - val_accuracy: 0.6242\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.4505 - accuracy: 0.8376 - val_loss: 3.0406 - val_accuracy: 0.6251\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 1s 195ms/step - loss: 0.5246 - accuracy: 0.7934 - val_loss: 3.0493 - val_accuracy: 0.6247\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 1s 178ms/step - loss: 0.5159 - accuracy: 0.7989 - val_loss: 3.0712 - val_accuracy: 0.6242\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 1s 182ms/step - loss: 0.5435 - accuracy: 0.7841 - val_loss: 3.0893 - val_accuracy: 0.6242\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 1s 220ms/step - loss: 0.5052 - accuracy: 0.8118 - val_loss: 3.1087 - val_accuracy: 0.6242\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 1s 253ms/step - loss: 0.5577 - accuracy: 0.7768 - val_loss: 3.1179 - val_accuracy: 0.6228\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 1s 181ms/step - loss: 0.4667 - accuracy: 0.8155 - val_loss: 3.1256 - val_accuracy: 0.6223\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 1s 201ms/step - loss: 0.5306 - accuracy: 0.7934 - val_loss: 3.1426 - val_accuracy: 0.6219\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 1s 163ms/step - loss: 0.5399 - accuracy: 0.7823 - val_loss: 3.1570 - val_accuracy: 0.6223\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 1s 141ms/step - loss: 0.4880 - accuracy: 0.7970 - val_loss: 3.1749 - val_accuracy: 0.6219\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 1s 196ms/step - loss: 0.5469 - accuracy: 0.7841 - val_loss: 3.1995 - val_accuracy: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa81f8a6110>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_reg.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    trainSet.batch(128),\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    validation_data=validSet.batch(128),\n",
    "    callbacks=[TensorBoard(log_dir='/tmp/nsl')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is better for regularised graph and we expect this to outperform vanilla graphs for a large number of epochs. Also note that performance increases as ratio of labeled data increases (more training data)/ ratio of supervised/unsupervised parts of our graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planetoid\n",
    "Extend graph regularisation in order to account for higher-order proximities, we have Planetoid (Predicting labels and neighbours with emebddings trasductively or inductively from data), extends skip-gram for compute node embeddings to incorporate node-label information. Skip-gram methods are based on generating random walks through a graph then using the generated sequences to learn embeddings via a skip-gram model, we modify for supervised loss; where embeddings are fed to the following:\n",
    "-  Softmax layer to predict graph context of sampled random-walk sequences\n",
    "-  Set of hidden layers that combine together with hidden layers derived from node features to predict class labels\n",
    "\n",
    "The cost function to be minimised is composed of supervised and unsupervised loss $L_s$, $L_u$ respectively. The unsupervised loss is analgous to the one used with skip-gram with negative sampling where the supervised loss minimises the conditional probability.\n",
    "\n",
    "$L_s = -\\sum\\limits_{i \\in L} logP(y_i | x_i, e_i)$\n",
    "\n",
    "However this formulation is transductive as it requires samples belonging to the graph to be applied; in semi-supervised task this can efficiently be used to predict labels for unlabeled examples. However, cannot be used for unobserved samples. There is an inductive version of Planetoid by parameterising the embeddings as a function of the node features, via dedicated connected layers.\n",
    "\n",
    "## Graph CNNs\n",
    "Learn graph/node representations that can accurately predict node/graph labels. Note that the encoding function remains the same, what we change is the objective in a supervised setting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Each graph represents a protein and graph labels represent whether they are are enzymes or non-enzymes. The dataset includes 1113 graphs with 39 nodes and 73 edges on average for each graph. Graph nodes have 4 attributes (including a one-hot encoding of their label), and each graph is labelled as belonging to 1 of 2 classes."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from stellargraph import datasets # uses tf.Keras in backend\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dataset = datasets.PROTEINS()\n",
    "display(HTML(dataset.description))\n",
    "graphs, graph_labels = dataset.load()\n",
    "\n",
    "labels = graph_labels.to_numpy(dtype=int)\n",
    "\n",
    "# necessary for converting default string labels to int\n",
    "graph_labels = pd.get_dummies(graph_labels, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "# PaddedGraphGenerator automatically resolves differences in number of nodes by using padding\n",
    "generator = PaddedGraphGenerator(graphs=graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.layer import DeepGraphCNN\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "\n",
    "nrows = 35  # the number of rows for the output tensor\n",
    "layer_dims = [32, 32, 32, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_dims,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=nrows,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "gnn_inp, gnn_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat backbone to one-dimensional (1D) convolutional layers\n",
    "\n",
    "# necessary to connect backbone to head\n",
    "gnn_inp, gnn_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "# head part of model(classification)\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_dims), strides=sum(layer_dims))(gnn_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "x_out = Flatten()(x_out)\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=gnn_inp, outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, None, 4)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, None, 4)      0           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_12 (GraphConv (None, None, 32)     128         dropout_16[1][0]                 \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, None, 32)     0           graph_convolution_12[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_13 (GraphConv (None, None, 32)     1024        dropout_17[1][0]                 \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, None, 32)     0           graph_convolution_13[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_14 (GraphConv (None, None, 32)     1024        dropout_18[1][0]                 \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, None, 32)     0           graph_convolution_14[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_15 (GraphConv (None, None, 1)      32          dropout_19[1][0]                 \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_5 (TensorFlo [(None, None, 97)]   0           graph_convolution_12[1][0]       \n",
      "                                                                 graph_convolution_13[1][0]       \n",
      "                                                                 graph_convolution_14[1][0]       \n",
      "                                                                 graph_convolution_15[1][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sort_pooling_3 (SortPooling)    (None, 3395, 1)      0           tf_op_layer_concat_5[0][0]       \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 35, 16)       1568        sort_pooling_3[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 17, 16)       0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 13, 32)       2592        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 416)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          53376       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 128)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            129         dropout_21[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 59,873\n",
      "Trainable params: 59,873\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    graph_labels, test_size=.3, stratify=labels,\n",
    ")\n",
    "\n",
    "gen = PaddedGraphGenerator(graphs=graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=50,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    symmetric_normalization=False,\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 14s 875ms/step - loss: 0.5051 - acc: 0.7510 - val_loss: 0.5380 - val_acc: 0.7335\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 14s 844ms/step - loss: 0.5090 - acc: 0.7625 - val_loss: 0.5349 - val_acc: 0.7365\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 14s 900ms/step - loss: 0.4969 - acc: 0.7664 - val_loss: 0.5342 - val_acc: 0.7335\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 14s 862ms/step - loss: 0.4982 - acc: 0.7689 - val_loss: 0.5379 - val_acc: 0.7395\n",
      "Epoch 5/100\n",
      "14/16 [=========================>....] - ETA: 1s - loss: 0.5154 - acc: 0.7452"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-063906152ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification with GraphSage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Cora()\n",
    "G, nodes = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, test_nodes = train_test_split(nodes, train_size=0.1, test_size=None, stratify=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# convert using one-hot representation; often used for classification tasks and usually leads to better performance\n",
    "label_encoding = preprocessing.LabelBinarizer()\n",
    "train_labels = label_encoding.fit_transform(train_nodes)\n",
    "test_labels = label_encoding.transform(test_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import GraphSAGENodeGenerator\n",
    "\n",
    "batchsize = 50\n",
    "n_samples = [10, 5, 7]\n",
    "generator = GraphSAGENodeGenerator(G, batchsize, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.layer import GraphSAGE\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "graphsage_model = GraphSAGE(\n",
    "    layer_sizes=[32, 32, 16], generator=generator, bias=True, dropout=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_inp, gnn_out = graphsage_model.in_out_tensors()\n",
    "outputs = Dense(units=train_labels.shape[1], activation=\"softmax\")(gnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs=gnn_inp, outputs=outputs)\n",
    "model.compile(optimizer=Adam(lr=0.003), loss=categorical_crossentropy, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use flow function of generator for feeding model with train and test set\n",
    "train_gen = generator.flow(train_nodes.index, train_labels, shuffle=True)\n",
    "test_gen = generator.flow(test_nodes.index, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6/6 - 31s - loss: 1.9507 - acc: 0.2037 - val_loss: 1.8206 - val_acc: 0.3089\n",
      "Epoch 2/20\n",
      "6/6 - 30s - loss: 1.8350 - acc: 0.3111 - val_loss: 1.7468 - val_acc: 0.3175\n",
      "Epoch 3/20\n",
      "6/6 - 29s - loss: 1.8004 - acc: 0.3444 - val_loss: 1.6477 - val_acc: 0.4241\n",
      "Epoch 4/20\n",
      "6/6 - 29s - loss: 1.7028 - acc: 0.4333 - val_loss: 1.5628 - val_acc: 0.5509\n",
      "Epoch 5/20\n",
      "6/6 - 29s - loss: 1.6363 - acc: 0.5407 - val_loss: 1.4928 - val_acc: 0.5956\n",
      "Epoch 6/20\n",
      "6/6 - 30s - loss: 1.5909 - acc: 0.5556 - val_loss: 1.4140 - val_acc: 0.6341\n",
      "Epoch 7/20\n",
      "6/6 - 29s - loss: 1.5096 - acc: 0.5815 - val_loss: 1.3472 - val_acc: 0.6645\n",
      "Epoch 8/20\n",
      "6/6 - 30s - loss: 1.4286 - acc: 0.6481 - val_loss: 1.2907 - val_acc: 0.6743\n",
      "Epoch 9/20\n",
      "6/6 - 30s - loss: 1.3877 - acc: 0.6741 - val_loss: 1.2615 - val_acc: 0.6661\n",
      "Epoch 10/20\n",
      "6/6 - 29s - loss: 1.3119 - acc: 0.7111 - val_loss: 1.1991 - val_acc: 0.6973\n",
      "Epoch 11/20\n",
      "6/6 - 29s - loss: 1.2665 - acc: 0.7074 - val_loss: 1.1573 - val_acc: 0.7112\n",
      "Epoch 12/20\n",
      "6/6 - 29s - loss: 1.1916 - acc: 0.7333 - val_loss: 1.1234 - val_acc: 0.7153\n",
      "Epoch 13/20\n",
      "6/6 - 30s - loss: 1.1551 - acc: 0.7741 - val_loss: 1.0962 - val_acc: 0.7133\n",
      "Epoch 14/20\n",
      "6/6 - 30s - loss: 1.1166 - acc: 0.7741 - val_loss: 1.0803 - val_acc: 0.7129\n",
      "Epoch 15/20\n",
      "6/6 - 30s - loss: 1.0636 - acc: 0.8111 - val_loss: 1.0405 - val_acc: 0.7375\n",
      "Epoch 16/20\n",
      "6/6 - 30s - loss: 1.0006 - acc: 0.8148 - val_loss: 1.0172 - val_acc: 0.7469\n",
      "Epoch 17/20\n",
      "6/6 - 31s - loss: 0.9624 - acc: 0.8148 - val_loss: 0.9937 - val_acc: 0.7547\n",
      "Epoch 18/20\n",
      "6/6 - 29s - loss: 0.9329 - acc: 0.8259 - val_loss: 0.9858 - val_acc: 0.7543\n",
      "Epoch 19/20\n",
      "6/6 - 30s - loss: 0.9052 - acc: 0.8556 - val_loss: 0.9555 - val_acc: 0.7613\n",
      "Epoch 20/20\n",
      "6/6 - 30s - loss: 0.8675 - acc: 0.8556 - val_loss: 0.9385 - val_acc: 0.7666\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_gen, epochs=20, validation_data=test_gen, verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
